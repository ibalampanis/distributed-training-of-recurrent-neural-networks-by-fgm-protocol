\chapter{Conclusions}\label{ch:conclusions}

\section{Contribution}\label{sec:conclusion}

Our recommended method for distributed DL achieves high predictive performance, yet needs essentially less communication than GM\@.
Furthermore, the method handles not only the learning algorithm but also the optimizer as black-boxes.
In this work, I proved that FGM is better than GM for distributed DL learning and especially using LSTM networks, a subset of the Recurrent Neural Networks.
I tested this architecture on solving two types of problems, classification, and sentiment analysis.
In both cases, the results were impressive.
But if we have to choose one of these two for which the architecture is more suitable, the answer is the NLP problem.
Taking into account the difficulty of both problems, our architecture reacted almost in the same way in both cases.

In the second phase, I compared the two functions with each other.
The results revealed that SF2 is much cheaper than SF1 in terms of network cost, but the latter achieves better accuracy on the prediction.
Of course, this difference is not so important as to make us prefer it.
Therefore, sacrificing minimal accuracy in the model, we choose SF2 as the best for distributed deep learning.

\section{Future Work}\label{sec:future-work}

In this work, I simulated a scenario calculating the network traffic cost of the training process of RNN by the GM and FGM protocol.
We know this time in practice that the FGM protocol is more efficient than GM.
Thus, a future direction would be an actual system that uses FGM to train these networks.
Recently, Sofia Kampioti~\cite{kampioti_sofia__thesis_2020} implemented such a system to train an ML model for classification purposes using the Support Vector Machines (SVM) algorithm.

Another future direction would be the usage of the rebalancing version of FGM on RNN training.
Using the rebalancing version, we can undoubtedly achieve much more efficient training concerning the network cost.

Last, in this project, we made offline learning.
Future work could attempt to make this process online, taking into consideration some meaning like Concept Drift.
An online learning algorithm can resolve some issues like concept changes.
To make this more specific, in this task I used the food reviews as training samples.
In an online learning system, we could change the concept of training samples to cloth reviews without accepting a large reduction in the forecast performance.
