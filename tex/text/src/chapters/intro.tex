\chapter{Introduction}\label{ch:introduction}

Nowadays, deep neural networks are trained on ever-growing data corpora.
As a result, distributed training schemes are becoming increasingly important.
A major issue in distributed training is the limited communication bandwidth between contributing nodes or prohibitive communication costs in general.
Many pieces of research have made a try on distributed deep learning, but very few have considered the enormous network traffic costs that such a style requires.
Deep learning methods have proved to have strong predictive performance but on the other hand, a complex learning process.
Opportunely, the training of artificial neural networks uses algorithms like Gradient Descent decreasing their loss, a fact that leads to convenience to distribute their learning procedure.
In this work, we focus on distributing the learning process of Recurrent Neural Networks, while minimizing the communication of the remote sites.

\section{Related Work and Motivation}\label{sec:related-work-and-motivation}

The first tries for Distributed Machine Learning (DML) or Deep Learning were done by the parameter server method~\cite{li_scaling_2014}.
This structure of this method has nodes and a parameter server.
The central idea is when some batches of data or some real training time have passed, nodes synchronize with the server sending their parameters.
Then, the server aggregates all these model parameters and send back to nodes the fresh one to continue the learning process.

In 2019, Konidaris~\cite{konidaris_distributed_2019} used the GM~\cite{sharfman_geometric_2007}~\cite{sharfman_aggregate_2007}
and the FGM~\cite{garofalakis_sketch-based_2013}~\cite{garofalakis_distributed_nodate}~\cite{samoladas_functional_nodate} to train one other architecture of Artificial Neural Networks, the Convolutional Neural Networks.
The work had unbelievable results regarding the gap of the network cost between the two methods.
So, there I found the motivation to use these two methods, this time to train an architecture that comprises Recurrent Neural Networks.
Besides, while searching for my diploma thesis subject, I do not found a lot of works on distributed training of RNNs.
Making work with successful results translates to a valuable source for other people in the Machine Learning community.

\section{Thesis Goal}\label{sec:our goal}

This work aims to provide a comparison of two methods for a distributed training process of Recurrent Neural Networks.
The comparison is about the network cost of these two methods.
I focus on two supervised learning problems, Classification and Natural Language Processing, using a subset of the RNN architecture, the Long-Short Term Memory Networks as learning models.
The distributed learning process will be achieved by two geometric monitoring methods, the GM and the FGM\@.
Furthermore, I am going to compare two safe function, to decide which is better for distributed Deep Learning purposes.
I will refer to these functions in later chapters.


\section{Thesis Overview}\label{sec:thesis-overview}

This section summarizes the structure of this diploma thesis.

\textbf{Chapter~\ref{ch:theoretical-background}} presents the background you need to understand the meanings of this work.
At first, section \textbf{\ref{sec:machine-learning}} refers to Machine Learning Paradigms and Deep Learning.
It makes a further reference to Recurrent Neural Networks because my Deep Learning model is based on there.
Finally, section \textbf{\ref{sec:geometric-monitoring-methods}} introduces the two Geometric Monitoring methods which I have implemented and compared each other.

\textbf{Chapter~\ref{ch:implementation}} presents the tools that helped me to implement the above algorithms.
It also presents the structure and setup of the Deep Learning model.
Lastly, explains in detail the implementation by giving some pseudocodes to make it easier to understand.

\textbf{Chapter~\ref{ch:experimental-results}}, explains the results that came off from the experimental phase.

\textbf{Chapter~\ref{ch:conclusions}}, concludes this work as well as proposes some ideas for further research in the future.

In the end, \textbf{Appendix~\ref{ch:abbreviations}} you can find the abbreviations I have used in my text,
while \textbf{Appendix~\ref{ch:detailed-experimental-results}} provides the tables with the numerical data that resulted from the experimental phase.